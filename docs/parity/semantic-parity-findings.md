# Semantic Parity Validation Findings

## Overview

This document captures findings from WP1.5: Semantic Parity Validation, which validates that Rust-written metadata matches Iceberg-Java/Spark expectations **exactly**, not just "is readable".

**Status:** In Progress
**Sprint:** WP1.5 (Production Parity Hardening)
**Test File:** `crates/integration_tests/tests/shared_tests/semantic_parity_test.rs`

## Validation Approach

### Test Infrastructure

We use **paired tables** for comparison:
- `parity_*_rust`: Table where Rust performs the operation
- `parity_*_spark`: Table where Spark performs the identical operation

Both tables start with identical initial state, then we compare the resulting metadata structures field-by-field.

### Comparison Categories

| Category | Expectation |
|----------|-------------|
| Core counts (`added-data-files`, `added-records`) | MUST match exactly |
| Operation type | MUST match exactly |
| Total counts (`total-data-files`, `total-records`) | MUST match exactly |
| Size fields (`added-files-size`, `total-files-size`) | May differ (compression) |
| Parent snapshot linkage (presence) | MUST match (Some/None) |
| IDs (`snapshot-id`) | Expected to differ |
| Timestamps (`committed-at`) | Format/precision documented; values differ |

---

## Snapshot Summary Field Validation

### Fields Validated

| Field | Purpose | Rust Source |
|-------|---------|-------------|
| `added-data-files` | Count of new data files | `SnapshotSummaryCollector` |
| `deleted-data-files` | Count of removed data files | `SnapshotSummaryCollector` |
| `added-records` | Count of new rows | `SnapshotSummaryCollector` |
| `deleted-records` | Count of removed rows | `SnapshotSummaryCollector` |
| `added-delete-files` | Count of new delete files | `SnapshotSummaryCollector` |
| `removed-delete-files` | Count of removed delete files | `SnapshotSummaryCollector` |
| `added-position-deletes` | Count of position delete entries | `SnapshotSummaryCollector` |
| `added-equality-deletes` | Count of equality delete entries | `SnapshotSummaryCollector` |
| `removed-files-size` | Size of removed files | `SnapshotSummaryCollector` |
| `total-data-files` | Running total of data files | `update_snapshot_summaries` |
| `total-delete-files` | Running total of delete files | `update_snapshot_summaries` |
| `total-records` | Running total of records | `update_snapshot_summaries` |
| `total-files-size` | Running total of file size | `update_snapshot_summaries` |
| `total-position-deletes` | Running total of position deletes | `update_snapshot_summaries` |
| `total-equality-deletes` | Running total of equality deletes | `update_snapshot_summaries` |
| `changed-partition-count` | Partitions affected | `SnapshotSummaryCollector` |

### Implementation

Summary fields are generated by:
1. `SnapshotSummaryCollector::build()` - Collects per-operation metrics
2. `update_snapshot_summaries()` - Updates running totals from previous snapshot

**Key file:** `crates/iceberg/src/spec/snapshot_summary.rs`

---

## Commit Metadata Consistency

We validate commit metadata fields that are not summary map entries:

| Field | Expectation | Notes |
|-------|-------------|-------|
| Operation | MUST match | Compared via snapshot summary `operation` |
| Parent snapshot presence | MUST match | Snapshot IDs differ per table, but presence should align |
| Timestamp format/precision | Documented | Rust stores epoch ms; Spark returns timestamp strings |

---

## Manifest Entry Structure

### Required Fields

All manifest entries MUST contain:

| Field | Description | Iceberg Spec |
|-------|-------------|--------------|
| `status` | Entry status (0=existing, 1=added, 2=deleted) | Required |
| `snapshot_id` | Snapshot that added this entry | Required |
| `sequence_number` | Sequence number of the commit | V2+ |
| `file_sequence_number` | Sequence number of the file | V2+ |

### Data File Fields

Each data file entry MUST contain:

| Field | Description |
|-------|-------------|
| `content` | File type (0=data, 1=position-deletes, 2=equality-deletes) |
| `file_path` | Path to the data file |
| `file_format` | Format (parquet, orc, avro) |
| `partition` | Partition values |
| `record_count` | Number of records in file |
| `file_size_in_bytes` | Size of file |

Partition values are normalized into JSON maps for comparison:
- Rust: `partition_struct_to_json` uses the manifest's partition type + `Literal::try_into_json`
- Spark: `Row.asDict(recursive=True)` from the `entries` metadata table

Ordering checks compare status/content sequences and partition multisets to flag any drift.

---

## Edge Case Behavior Matrix

### NULL Handling

| Scenario | Expected Behavior | Test |
|----------|-------------------|------|
| DELETE WHERE col IS NULL | Delete rows with NULL | `test_semantic_parity_null_handling` |
| NULL partition values | Should be serialized correctly | `test_semantic_parity_empty_partition_delete` |
| Predicates with NULL comparisons | Three-valued logic | Pending |

### Empty Results

| Scenario | Expected Behavior | Test |
|----------|-------------------|------|
| DELETE that matches nothing | No snapshot OR no-op snapshot | `test_edge_case_empty_delete` |
| Empty result sets | Graceful handling | Pending |

### Boundary Values

| Scenario | Status |
|----------|--------|
| Min/max integers | `test_semantic_parity_boundary_values` |
| Empty strings | `test_semantic_parity_boundary_values` |
| Zero-length binary | Pending |

### Empty Partitions

| Scenario | Expected Behavior | Test |
|----------|-------------------|------|
| DELETE removes entire partition | Partition handled consistently | `test_semantic_parity_empty_partition_delete` |

---

## Error Rejection Parity

Status: **Pending**. The Rust API does not yet expose schema evolution operations needed
to validate invalid schema changes or incompatible type promotions against Spark. Once
schema update APIs are available, add parity tests for:

- Invalid schema changes (e.g., incompatible type promotion)
- Constraint violations where applicable

---

## Known Divergences

### Documented Differences

| Field | Difference | Rationale |
|-------|------------|-----------|
| `added-files-size` | May differ slightly | Compression settings, writer options |
| `removed-files-size` | May differ slightly | Compression settings |
| `total-files-size` | May differ slightly | Cumulative size differences |
| `snapshot-id` | Always differs | Snapshot IDs are table-specific |
| `committed-at` | Format differs | Rust uses epoch ms; Spark returns timestamp string |

### Investigated Issues

_This section will be populated after running tests against live containers._

---

## Test Results

### DELETE Operation Parity

**Tables:** `parity_delete_rust`, `parity_delete_spark`
**Operation:** `DELETE WHERE value > 300` (deletes 2 rows)

| Field | Rust | Spark | Match | Notes |
|-------|------|-------|-------|-------|
| operation | TBD | TBD | TBD | Run tests to populate |
| added-delete-files | TBD | TBD | TBD | |
| added-position-deletes | TBD | TBD | TBD | |
| total-data-files | TBD | TBD | TBD | |

### NULL Handling Parity

**Tables:** `parity_null_rust`, `parity_null_spark`
**Operation:** `DELETE WHERE name IS NULL` (deletes 2 rows)

| Field | Rust | Spark | Match | Notes |
|-------|------|-------|-------|-------|
| operation | TBD | TBD | TBD | Run tests to populate |

### UPDATE Operation Parity

**Tables:** `parity_update_rust`, `parity_update_spark`
**Operation:** `UPDATE status='done' WHERE id > 2` (updates 2 rows)

| Field | Rust | Spark | Match | Notes |
|-------|------|-------|-------|-------|
| operation | TBD | TBD | TBD | Run tests to populate |
| added-records | TBD | TBD | TBD | |
| deleted-records | TBD | TBD | TBD | |
| added-delete-files | TBD | TBD | TBD | |

### MERGE Operation Parity

**Tables:** `parity_merge_rust`, `parity_merge_spark`
**Operation:** Update ids 1/3 and insert id 5

| Field | Rust | Spark | Match | Notes |
|-------|------|-------|-------|-------|
| operation | TBD | TBD | TBD | Run tests to populate |
| added-records | TBD | TBD | TBD | |
| deleted-records | TBD | TBD | TBD | |
| added-delete-files | TBD | TBD | TBD | |

### COMPACTION Parity

**Tables:** `parity_compact_rust`, `parity_compact_spark`
**Operation:** Binpack compaction / rewrite_data_files

| Field | Rust | Spark | Match | Notes |
|-------|------|-------|-------|-------|
| operation | TBD | TBD | TBD | Run tests to populate |
| added-data-files | TBD | TBD | TBD | |
| deleted-data-files | TBD | TBD | TBD | |
| removed-files-size | TBD | TBD | TBD | Expected size variance |

### EXPIRE SNAPSHOTS Parity

**Tables:** `parity_expire_rust`, `parity_expire_spark`
**Operation:** Expire snapshots (retain last 1)

| Field | Rust | Spark | Match | Notes |
|-------|------|-------|-------|-------|
| operation | TBD | TBD | TBD | Run tests to populate |
| total-data-files | TBD | TBD | TBD | |
| total-records | TBD | TBD | TBD | |

### Boundary Values Parity

**Tables:** `parity_boundary_rust`, `parity_boundary_spark`
**Operation:** `DELETE WHERE id = -2147483648 OR name = ''`

| Field | Rust | Spark | Match | Notes |
|-------|------|-------|-------|-------|
| operation | TBD | TBD | TBD | Run tests to populate |
| deleted-records | TBD | TBD | TBD | |

### Empty Partition + Manifest Entry Parity

**Tables:** `parity_partition_rust`, `parity_partition_spark`
**Operation:** `DELETE WHERE category = 'B'` (empties one partition)

Snapshot summary:

| Field | Rust | Spark | Match | Notes |
|-------|------|-------|-------|-------|
| operation | TBD | TBD | TBD | Run tests to populate |
| changed-partition-count | TBD | TBD | TBD | |

Manifest entry parity:

| Field | Rust | Spark | Match | Notes |
|-------|------|-------|-------|-------|
| entry-count | TBD | TBD | TBD | |
| status-sequence | TBD | TBD | TBD | Ordering check |
| content-sequence | TBD | TBD | TBD | |
| partition-values | TBD | TBD | TBD | JSON map comparison |

---

## How to Run Tests

```bash
# Start the test containers
cd crates/integration_tests/testdata
docker compose -p testdata up -d --wait

# Run semantic parity tests
ICEBERG_SPARK_CONTAINER=testdata-spark-iceberg-1 cargo test --test integration_test semantic_parity -- --nocapture

# Run specific test
ICEBERG_SPARK_CONTAINER=testdata-spark-iceberg-1 cargo test --test integration_test test_semantic_parity_delete -- --nocapture
```

---

## Recommendations

### Completed

1. [x] Added `snapshot_summary` + `execute_sql` validation types to validate.py
2. [x] Added Rust helpers for snapshot summary + manifest entry extraction
3. [x] Created comparison infrastructure in semantic_parity_test.rs
4. [x] Implemented DELETE + NULL handling parity tests
5. [x] Implemented UPDATE, MERGE, COMPACTION, EXPIRE parity tests
6. [x] Implemented boundary value + empty partition parity tests
7. [x] Implemented manifest entry parity comparison (structure + partition serialization)

### To Do

1. [ ] Run tests against live containers to populate findings
2. [ ] Investigate any divergences found
3. [ ] Add error rejection parity tests (invalid schema changes, incompatible type promotion)
4. [ ] Add zero-length binary edge case test

---

## References

- **Iceberg Spec:** https://iceberg.apache.org/spec/
- **Snapshot Summary fields:** `crates/iceberg/src/spec/snapshot_summary.rs`
- **Manifest structure:** `crates/iceberg/src/spec/manifest/`
- **Test file:** `crates/integration_tests/tests/shared_tests/semantic_parity_test.rs`
